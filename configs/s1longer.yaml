train:
    seed: 1234
    epochs: 1500
    batch_size: 6
    save_every_n_epoch: 2
    precision: 16-mixed
    gradient_clip: 1.0
    if_save_every_weights: true
    if_save_latest: false
    half_weights_save_dir: "./GPT_weights/"
    exp_name: "DailyTalk"
optimizer:
    lr: 0.01
    lr_init: 0.00001
    lr_end: 0.0001
    warmup_steps: 2000
    decay_steps: 40000
data:
    max_eval_sample: 8
    max_sec: 54
    num_workers: 2 
    pad_val: 1024 
model:
    vocab_size: 1025
    phoneme_vocab_size: 512
    embedding_dim: 512
    hidden_dim: 512
    head: 16
    linear_units: 2048
    n_layer: 24
    dropout: 0
    EOS: 1024
    random_bert: 0
inference:
    top_k: 5

train_semantic_path: "I:\\GPT-Talker\\datasets\\processed\\DailyTalk\\6-name2semantic.tsv"
train_phoneme_path: "I:\\GPT-Talker\\datasets\\processed\\DailyTalk\\2-name2text.txt"
output_dir: "I:\\GPT-Talker\\log\\DailyTalk\\logs_s1"
pretrained_s1: "pretrained_models/s1bert25hz-2kh-longer-epoch=68e-step=50232.ckpt"